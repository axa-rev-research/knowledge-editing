{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f0dfc7e7d64e7187e8535fab912f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from huggingface_hub import notebook_login\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import ast \n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "import json\n",
    "import ot\n",
    "notebook_login()\n",
    "import sys\n",
    "sys.path.append('../') \n",
    "from SAKE.distributions import *\n",
    "from SAKE.threshold import *\n",
    "from SAKE.edit import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available(): device = torch.device(\"mps\")\n",
    "else: device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../data/cf/example.json') as f:\n",
    "    cf = json.load(f)\n",
    "\n",
    "# with open('../data/cf/counterfact.json') as f:\n",
    "#     cf = json.load(f)\n",
    "\n",
    "indexes = (0,5) # change if you want to do other edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:777: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model_name = 'gpt2-xl'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, output_hidden_states = True).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:03<00:00,  1.60it/s]\n"
     ]
    }
   ],
   "source": [
    "maps = []\n",
    "for i in tqdm(range(indexes[0], indexes[1])):\n",
    "    e = cf[i]\n",
    "    x = np.concatenate([e['forced_source_embs'], e['source_embs'], e['target_embs']])\n",
    "    x_source = x[:2*len(e['forced_source_embs'])]\n",
    "    x_target = x[2*len(e['forced_source_embs']):]\n",
    "    ot_linear = ot.da.LinearTransport(reg=1e-2)\n",
    "    ot_linear.fit(Xs=x_source, Xt=x_target)\n",
    "    maps.append(ot_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_mean_embs = []\n",
    "target_mean_embs = []\n",
    "for i in range(indexes[0], indexes[1]):\n",
    "    source_mean_embs.append(np.mean(cf[i]['source_embs'], axis = 0).tolist())\n",
    "    target_mean_embs.append(np.mean(cf[i]['target_embs'], axis = 0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_diffs = []\n",
    "for i in range(indexes[0], indexes[1]):\n",
    "    mean_diffs.append(np.array(target_mean_embs[i]) - np.array(source_mean_embs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:00<00:00,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English\n",
      "Islam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:00<00:00,  6.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piano\n",
      "Sweden\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  6.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manila\n",
      "Score:  5 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dI_a = 0\n",
    "total = 0\n",
    "for i in tqdm(range(indexes[0],indexes[1])):\n",
    "    total += 1\n",
    "    prompt = cf[i]['requested_rewrite']['prompt'].replace('{}', cf[i]['requested_rewrite']['subject'])\n",
    "    doubt = prompt + \" \" + cf[i]['requested_rewrite']['target_new']['str'] + \"??? No, it can't be, This contradicts my understanding. It is pretty clear that the answer to '\" +  prompt + \"' is something else. Rethink and give the correct answer:\"\n",
    "    input_ids = tokenizer(doubt, return_tensors=\"pt\").to('mps').input_ids\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        output = model(input_ids)\n",
    "\n",
    "    last_h = output.hidden_states[-1][:,-1:,:].view(-1).cpu()\n",
    "    pre_sim, max_index = hid_threshold(last_h, [source_mean_embs[i]], 1000, dist_type = \"euc\")\n",
    "\n",
    "    if max_index is not None:\n",
    "        last_h = torch.tensor(maps[i].transform(Xs=last_h), dtype = torch.float32)\n",
    "        # last_h += torch.tensor(mean_diffs[i], dtype = torch.float16)\n",
    "        last_h = last_h.view(-1)\n",
    "    else:\n",
    "        last_h = output.hidden_states[-1][:,-1,:].cpu()\n",
    "    \n",
    "    logits = torch.matmul(last_h, model.lm_head.weight.T.cpu())\n",
    "    next_token_probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    if next_token_probs.shape[0] == 1:\n",
    "        next_token_probs = next_token_probs[0]\n",
    "        \n",
    "    new_object_token = cf[i]['requested_rewrite']['target_new']['str']\n",
    "    old_object_token = cf[i]['requested_rewrite']['target_true']['str']\n",
    "\n",
    "    next_token_id = torch.argmax(next_token_probs, dim = -1)\n",
    "    next_token = tokenizer.decode(next_token_id).strip()\n",
    "\n",
    "    if new_object_token.startswith(next_token) and next_token != \"\":\n",
    "        dI_a += 1\n",
    "\n",
    "print(\"Score: \", dI_a, \"out of\", total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:00<00:00,  5.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mother tongue of Danielle Darrieux is English\n",
      "The official religion of Edwin of Northumbria is Islam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:00<00:00,  6.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toko Yasuda, the piano\n",
      "Autonomous University of Madrid, which is located in Sweden\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  6.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the twin city of Lyon? It is Manila\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dII_a = 0\n",
    "for i in tqdm(range(indexes[0], indexes[1])):\n",
    "\n",
    "    prompt = cf[i]['requested_rewrite']['prompt'].replace('{}', cf[i]['requested_rewrite']['subject'])\n",
    "    doubt = prompt + \" \" + cf[i]['requested_rewrite']['target_new']['str'] + \"? Really? But I am pretty sure that \" + prompt + \" \" + cf[i]['requested_rewrite']['target_true']['str'] + \". Am I right? Yes, you are right. I apologize for any confusion. Reconsider that the answer to question \" + prompt + \"? should be\"\n",
    "    input_ids = tokenizer(doubt, return_tensors=\"pt\").to('mps').input_ids\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        output = model(input_ids)\n",
    "\n",
    "    last_h = output.hidden_states[-1][:,-1:,:].view(-1).cpu()\n",
    "    pre_sim, max_index = hid_threshold(last_h, [source_mean_embs[i]], 1000, dist_type = \"euc\")\n",
    "    \n",
    "    if max_index is not None:\n",
    "        last_h = torch.tensor(maps[i].transform(Xs=last_h), dtype = torch.float32)\n",
    "        # last_h += torch.tensor(mean_diffs[i], dtype = torch.float16)\n",
    "        last_h = last_h.view(-1)\n",
    "    else:\n",
    "        last_h = output.hidden_states[-1][:,-1,:].cpu()\n",
    "\n",
    "    logits = torch.matmul(last_h, model.lm_head.weight.T.cpu())\n",
    "    next_token_probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    if next_token_probs.shape[0] == 1:\n",
    "        next_token_probs = next_token_probs[0]\n",
    "        \n",
    "    new_object_token = cf[i]['requested_rewrite']['target_new']['str'] \n",
    "    old_object_token = cf[i]['requested_rewrite']['target_true']['str']\n",
    "\n",
    "    next_token_id = torch.argmax(next_token_probs, dim = -1)\n",
    "    next_token = tokenizer.decode(next_token_id).strip()\n",
    "    if new_object_token.startswith(next_token) and next_token != \"\":\n",
    "        dII_a += 1\n",
    "\n",
    "    print(prompt, next_token)\n",
    "print(dII_a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
