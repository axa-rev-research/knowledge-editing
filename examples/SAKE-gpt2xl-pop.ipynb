{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c0639e5d60f453f925b4a1cf02aebbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from huggingface_hub import notebook_login\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "import ast \n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "import json\n",
    "import ot\n",
    "import qwikidata\n",
    "from qwikidata.entity import WikidataItem\n",
    "from qwikidata.linked_data_interface import get_entity_dict_from_api\n",
    "import sys\n",
    "sys.path.append('../') \n",
    "from SAKE.distributions import *\n",
    "from SAKE.threshold import *\n",
    "from SAKE.edit import *\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available(): device = torch.device(\"mps\")\n",
    "else: device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you load the example dataset and want to try a few sentences only, no need to generate sentences later. You can also skip \"extract representations\" and \"save the dataset\" steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/ripple/popular.json') as f:\n",
    "#     pop = json.load(f)\n",
    "\n",
    "with open('../data/ripple/example.json') as f:\n",
    "    pop = json.load(f)\n",
    "\n",
    "indexes = (0,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [03:23<00:00, 20.36s/it]\n"
     ]
    }
   ],
   "source": [
    "pop = generate_paraphrases_popular(pop=pop, provider=\"anthropic\", api_key=\"your_api_key\", model=\"claude-3-5-sonnet-latest\", indexes=indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:17<00:00,  7.72s/it]\n"
     ]
    }
   ],
   "source": [
    "pop = generate_implications_popular(pop, provider=\"anthropic\", api_key=\"your_api_key\", model=\"claude-3-5-sonnet-latest\", indexes=indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The capital of the country of citizenship of Leonardo DiCaprio is',\n",
       " 'The currency of the country of citizenship of Leonardo DiCaprio is',\n",
       " 'The president of the country of citizenship of Leonardo DiCaprio is',\n",
       " 'The national anthem of the country of citizenship of Leonardo DiCaprio is',\n",
       " 'The dominant language of the country of citizenship of Leonardo DiCaprio is']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop[0]['comp_list'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cf (optional)\n",
    "with open('../data/ripple/example.json', 'w') as f:\n",
    "    json.dump(pop, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:777: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model_name = 'gpt2-xl'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, output_hidden_states = True).to('mps')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [08:19<00:00, 49.95s/it]\n"
     ]
    }
   ],
   "source": [
    "pop = extract_representations_popular(model=model, tokenizer=tokenizer, pop=pop, indexes=indexes, device='mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/ripple/example.json', 'w') as f:\n",
    "    json.dump(pop, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_mean_embs = []\n",
    "target_mean_embs = []\n",
    "for i in range(indexes[0], indexes[1]):\n",
    "    source_mean_embs.append(np.mean(np.concatenate([pop[i]['source_embs'], pop[i]['forced_source_embs']], axis = 0), axis = 0).tolist())\n",
    "    #source_mean_embs.append(np.mean(cf[i]['forced_source_embs'], axis = 0).tolist())\n",
    "    target_mean_embs.append(np.mean(pop[i]['target_embs'], axis = 0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_diffs = []\n",
    "for i in range(indexes[0], indexes[1]):\n",
    "    mean_diffs.append(np.array(target_mean_embs[i]) - np.array(source_mean_embs[i]))\n",
    "\n",
    "mean_diffs = [x.tolist() for x in mean_diffs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comp embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step is to keep embeddings corresponding to logical implications only when the difference vector is similar to the mean difference vector of the paraphrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 158.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# keep 10 highest correlated embeddings\n",
    "for i in tqdm(range(indexes[0], indexes[1])):\n",
    "    comp_diffs = []\n",
    "    for h in range(len(pop[i]['source_comp_embs'])):\n",
    "        comp_diffs.append(np.array(pop[i]['target_comp_embs'][h]) - np.array(pop[i]['source_comp_embs'][h]))\n",
    "\n",
    "    comp_diffs = [x.tolist() for x in comp_diffs]\n",
    "    # sorted_idx = sorted(range(len(comp_diffs)), key=lambda x: cosine_similarity(comp_diffs[x], mean_diffs[i]), reverse=True)\n",
    "    top_10_idx = sorted(range(len(comp_diffs)), key=lambda x: cosine_similarity(comp_diffs[x], mean_diffs[i]), reverse=True)[:10]\n",
    "    top_10_source_embs = [pop[i]['source_comp_embs'][x] for x in top_10_idx]\n",
    "    top_10_target_embs = [pop[i]['target_comp_embs'][x] for x in top_10_idx]\n",
    "    pop[i]['source_comp_embs'] = top_10_source_embs\n",
    "    pop[i]['target_comp_embs'] = top_10_target_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target comp\n",
    "for i in range(indexes[0], indexes[1]):\n",
    "    for h in range(len(pop[i]['target_comp_embs'])):\n",
    "        # for each target embedding, generate another one by adding small normal noise\n",
    "        target_last_h = pop[i]['target_comp_embs'][h]\n",
    "        for n in range(4):\n",
    "            rand = np.random.normal(0, 0.1, len(target_last_h))\n",
    "            pop[i]['target_comp_embs'].append(list(target_last_h + rand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source comp\n",
    "for i in range(indexes[0], indexes[1]):\n",
    "    for h in range(len(pop[i]['source_comp_embs'])):\n",
    "        # for each target embedding, generate another one by adding small normal noise\n",
    "        target_last_h = pop[i]['source_comp_embs'][h]\n",
    "        for n in range(4):\n",
    "            rand = np.random.normal(0, 0.1, len(target_last_h))\n",
    "            pop[i]['source_comp_embs'].append(list(target_last_h + rand))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute prompt embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:11<00:00,  1.14s/it]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "emb_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "prompt_embs = compute_prompt_means_popular(pop, emb_model, indexes=indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learn mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we learn the actual edits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:06<00:00,  1.65it/s]\n"
     ]
    }
   ],
   "source": [
    "maps = []\n",
    "for i in tqdm(range(indexes[0], indexes[1])):\n",
    "    e = pop[i]\n",
    "    x = np.concatenate([e['forced_source_embs'], e['source_embs'], e['source_comp_embs'], e['target_embs'], e['target_comp_embs']])\n",
    "    #x = np.concatenate([e['forced_source_embs'], e['source_embs'], e['target_embs'], e['target_embs']])\n",
    "    x_source = x[:2*len(e['forced_source_embs'])+len(e['source_comp_embs'])]\n",
    "    x_target = x[2*len(e['forced_source_embs'])+len(e['source_comp_embs']):]\n",
    "    ot_linear = ot.da.LinearTransport(reg=1e-2)\n",
    "    ot_linear.fit(Xs=x_source, Xt=x_target)\n",
    "    maps.append(ot_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 5 # this threshold is good for euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:13<00:00,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "Argmax score:  8 out of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "argmax_success = 0\n",
    "total = 0\n",
    "prob_indexes = []\n",
    "for n in tqdm(range(indexes[0], indexes[1])):\n",
    "    total += 1\n",
    "    target_new_id = pop[n]['edit']['target_id']\n",
    "    target_true_id = pop[n]['edit']['original_fact']['target_id']\n",
    "    target_new = WikidataItem(get_entity_dict_from_api(target_new_id)).get_label()\n",
    "    target_true = WikidataItem(get_entity_dict_from_api(target_true_id)).get_label()\n",
    "    prompt = pop[n]['edit']['prompt'][:-len(target_new)-2]\n",
    "    prompt_enc = emb_model.encode(prompt)\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device).input_ids \n",
    "       \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        output = model(input_ids)\n",
    "\n",
    "    last_h = output.hidden_states[-1][:,-1:,:].view(-1).cpu()\n",
    "    pre_sim, max_index = prompt_threshold(prompt_enc, [prompt_embs[n]], t, dist_type = \"euc\")\n",
    "    if max_index is not None:\n",
    "        last_h = torch.tensor(maps[n].transform(Xs=last_h), dtype = torch.float32)\n",
    "\n",
    "    logits = torch.matmul(last_h.view(-1), model.lm_head.weight.T.cpu())\n",
    "\n",
    "    next_token_probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    new_object_token = target_new\n",
    "    new_object_token_id = tokenizer.encode(new_object_token, add_special_tokens = False)[0]\n",
    "    new_object_token_prob = next_token_probs[new_object_token_id]   \n",
    "    old_object_token = target_true\n",
    "    old_object_token_id = tokenizer.encode(old_object_token, add_special_tokens = False)[0]\n",
    "    old_object_token_prob = next_token_probs[old_object_token_id]\n",
    "\n",
    "    if new_object_token_prob > old_object_token_prob:\n",
    "        prob_indexes.append(n)\n",
    "\n",
    "    next_token_id = torch.argmax(next_token_probs, dim = -1)\n",
    "    next_token = tokenizer.decode(next_token_id).strip()\n",
    "    \n",
    "    # check if next_token is at the beginning of the string of the new_objec_token\n",
    "    if new_object_token.startswith(next_token) :\n",
    "        argmax_success += 1\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(\"Argmax score: \", argmax_success, \"out of\", total)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comp1 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.47it/s]\n"
     ]
    }
   ],
   "source": [
    "compI_idxs = []\n",
    "for n in tqdm(range(indexes[0], indexes[1])):\n",
    "    idxs = []\n",
    "    for j in range(len(pop[n]['Compositionality_I'])):\n",
    "        cond_prompt = pop[n]['Compositionality_I'][j]['condition_queries'][0]['prompt']\n",
    "        cond_input_ids = tokenizer(cond_prompt, return_tensors=\"pt\").input_ids.to('mps')\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            cond_output = model(cond_input_ids)\n",
    "        cond_last_h = cond_output.hidden_states[-1][:,-1,:]\n",
    "        cond_logits = torch.matmul(cond_last_h, model.lm_head.weight.T)\n",
    "        cond_next_token_probs = torch.softmax(cond_logits, dim=-1)\n",
    "        cond_new_object_token = pop[n]['Compositionality_I'][j]['condition_queries'][0]['answers'][0]['value']\n",
    "        aliases = pop[n]['Compositionality_I'][j]['condition_queries'][0]['answers'][0]['aliases']\n",
    "        cond_next_token = tokenizer.decode(torch.argmax(cond_next_token_probs, dim = -1)).strip()\n",
    "        if any(s.startswith(cond_next_token) for s in [cond_new_object_token] + aliases) and cond_next_token != \"\":\n",
    "            idxs.append(j)\n",
    "    compI_idxs.append(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The official language of the country of citizenship of Leonardo DiCaprio is\n",
      "English\n",
      "The name of the capital city of the country of citizenship of Leonardo DiCaprio is\n",
      "Damascus\n",
      "The name of the head of state of the country of citizenship of Leonardo DiCaprio is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 16.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bashar\n",
      "------------------------------------\n",
      "Total number of edits:  3\n",
      "Argmax score:  0.6666666666666666 with 2 out of 3 edits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cI_argmax_success = 0\n",
    "count = 0\n",
    "for n in tqdm(range(indexes[0], indexes[1])):\n",
    "    if n not in prob_indexes:\n",
    "        continue\n",
    "    if pop[n]['Compositionality_I']:\n",
    "        for j in range(len(pop[n]['Compositionality_I'])):\n",
    "            if j not in compI_idxs[n]:\n",
    "                continue\n",
    "            count += 1\n",
    "            prompt = pop[n]['Compositionality_I'][j]['test_queries'][0]['prompt']\n",
    "            print(prompt)\n",
    "            prompt_enc = emb_model.encode(prompt) \n",
    "            input_ids = tokenizer(prompt, return_tensors=\"pt\").to('mps').input_ids\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                output = model(input_ids)\n",
    "\n",
    "            last_h = output.hidden_states[-1][:,-1:,:].view(-1).cpu()\n",
    "            pre_sim, max_index = prompt_threshold(prompt_enc, [prompt_embs[n]], t, dist_type = \"euc\")\n",
    "            if max_index is not None:\n",
    "                last_h = torch.tensor(maps[n].transform(Xs=last_h), dtype = torch.float32)\n",
    "\n",
    "            logits = torch.matmul(last_h.view(-1), model.lm_head.weight.T.cpu())\n",
    "\n",
    "            next_token_probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "            new_object_token = pop[n]['Compositionality_I'][j]['test_queries'][0]['answers'][0]['value']\n",
    "            new_object_token_id = tokenizer.encode(new_object_token, add_special_tokens = False)[0]\n",
    "            new_object_token_prob = next_token_probs[new_object_token_id]   \n",
    "            aliases = pop[n]['Compositionality_I'][j]['test_queries'][0]['answers'][0]['aliases']\n",
    "            next_token_id = torch.argmax(next_token_probs, dim = -1)\n",
    "            next_token = tokenizer.decode(next_token_id).strip()\n",
    "            print(next_token)\n",
    "            \n",
    "            # check if the string of new_objec_token or of any of the aliases begins with next_token\n",
    "            if any(s.startswith(next_token) for s in [new_object_token] + aliases) and next_token != \"\":\n",
    "                cI_argmax_success += 1\n",
    "print(\"------------------------------------\")\n",
    "print(\"Total number of edits: \", count)\n",
    "print(\"Argmax score: \", cI_argmax_success/count, \"with\", cI_argmax_success, \"out of\", count, \"edits\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comp2 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 59.81it/s]\n"
     ]
    }
   ],
   "source": [
    "compII_idxs = []\n",
    "for n in tqdm(range(indexes[0], indexes[1])):\n",
    "    idxs = []\n",
    "    for j in range(len(pop[n]['Compositionality_II'])):\n",
    "        cond_prompt = pop[n]['Compositionality_II'][j]['condition_queries'][0]['prompt']\n",
    "        cond_input_ids = tokenizer(cond_prompt, return_tensors=\"pt\").input_ids.to('mps')\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            cond_output = model(cond_input_ids)\n",
    "        cond_last_h = cond_output.hidden_states[-1][:,-1,:]\n",
    "        cond_logits = torch.matmul(cond_last_h, model.lm_head.weight.T)\n",
    "        cond_next_token_probs = torch.softmax(cond_logits, dim=-1)\n",
    "        cond_new_object_token = pop[n]['Compositionality_II'][j]['condition_queries'][0]['answers'][0]['value']\n",
    "        aliases = pop[n]['Compositionality_II'][j]['condition_queries'][0]['answers'][0]['aliases']\n",
    "        cond_next_token = tokenizer.decode(torch.argmax(cond_next_token_probs, dim = -1)).strip()\n",
    "        if any(s.startswith(cond_next_token) for s in [cond_new_object_token] + aliases) and cond_next_token != \"\":\n",
    "            idxs.append(j)\n",
    "    compII_idxs.append(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 192399.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "Total number of edits:  0\n",
      "Argmax score:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cII_argmax_success = 0\n",
    "count = 0\n",
    "for n in tqdm(range(indexes[0],indexes[1])):\n",
    "    if not pop[n]['Compositionality_II']:\n",
    "        continue\n",
    "    if n not in prob_indexes:\n",
    "        continue\n",
    "    for j in range(len(pop[n]['Compositionality_II'])):\n",
    "        if j not in compII_idxs[n]:\n",
    "            continue\n",
    "        count += 1\n",
    "        prompt = pop[n]['Compositionality_II'][j]['test_queries'][0]['prompt']\n",
    "        prompt_enc = emb_model.encode(prompt)\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "        for i in range(1):  \n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to('mps')\n",
    "            input_ids = inputs.input_ids\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                output = model(input_ids)\n",
    "\n",
    "            last_h = output.hidden_states[-1][:,-1:,:].view(-1).cpu()\n",
    "            pre_sim, max_index = prompt_threshold(prompt_enc, [prompt_embs[n]], t, dist_type = \"euc\")\n",
    "            if max_index is not None:\n",
    "                last_h = torch.tensor(maps[n].transform(Xs=last_h), dtype = torch.float32)\n",
    "\n",
    "            logits = torch.matmul(last_h.view(-1), model.lm_head.weight.T.cpu())\n",
    "            \n",
    "            next_token_probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "            new_object_token = pop[n]['Compositionality_II'][j]['test_queries'][0]['answers'][0]['value']\n",
    "            new_object_token_id = tokenizer.encode(new_object_token, add_special_tokens = False)[0]\n",
    "            new_object_token_prob = next_token_probs[new_object_token_id]   \n",
    "            aliases = pop[n]['Compositionality_II'][j]['test_queries'][0]['answers'][0]['aliases']\n",
    "            next_token_id = torch.argmax(next_token_probs, dim = -1)\n",
    "            next_token = tokenizer.decode(next_token_id).strip()\n",
    "                \n",
    "            # check if next_token is at the beginning of the string of the new_objec_token or in any of the elements of aliases, which is a list of strings\n",
    "            if any(s.startswith(next_token) for s in [new_object_token] + aliases) and next_token != \"\":\n",
    "                cII_argmax_success += 1\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(\"Total number of edits: \", count)\n",
    "print(\"Argmax score: \", cII_argmax_success)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rel spec test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:06<00:00,  1.59it/s]\n"
     ]
    }
   ],
   "source": [
    "rs_idxs = []\n",
    "for n in tqdm(range(indexes[0], indexes[1])):\n",
    "    idxs = []\n",
    "    for j in range(len(pop[n]['Relation_Specificity'])):\n",
    "        cond_prompt = pop[n]['Relation_Specificity'][j]['condition_queries'][0]['prompt']\n",
    "        cond_input_ids = tokenizer(cond_prompt, return_tensors=\"pt\").input_ids.to('mps')\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            cond_output = model(cond_input_ids)\n",
    "        cond_last_h = cond_output.hidden_states[-1][:,-1,:]\n",
    "        cond_logits = torch.matmul(cond_last_h, model.lm_head.weight.T)\n",
    "        cond_next_token_probs = torch.softmax(cond_logits, dim=-1)\n",
    "        cond_new_object_token = pop[n]['Relation_Specificity'][j]['condition_queries'][0]['answers'][0]['value']\n",
    "        aliases = pop[n]['Relation_Specificity'][j]['condition_queries'][0]['answers'][0]['aliases']\n",
    "        cond_next_token = tokenizer.decode(torch.argmax(cond_next_token_probs, dim = -1)).strip()\n",
    "        #print(cond_next_token)\n",
    "        if any(s.startswith(cond_next_token) for s in [cond_new_object_token] + aliases) and cond_next_token != \"\":\n",
    "            idxs.append(j)\n",
    "    rs_idxs.append(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 11.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of edits evaluated: 6\n",
      "Specificity score:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "rs_success = 0\n",
    "for n in tqdm(range(indexes[0], indexes[1])):\n",
    "    if n not in prob_indexes:\n",
    "        continue\n",
    "    for j in range(len(pop[n]['Relation_Specificity'])):\n",
    "        if j not in rs_idxs[n]:\n",
    "            continue\n",
    "        count += 1\n",
    "        prompt = pop[n]['Relation_Specificity'][j]['test_queries'][0]['prompt']\n",
    "        prompt_enc = emb_model.encode(prompt)\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").to('mps').input_ids\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            output = model(input_ids)\n",
    "\n",
    "        prompt_sim, prompt_idx = prompt_threshold(prompt_enc, [prompt_embs[n]], 5, dist_type = \"euc\")\n",
    "\n",
    "        if prompt_idx is None: \n",
    "            rs_success += 1\n",
    "\n",
    "        else: \n",
    "            last_h = output.hidden_states[-1][:,-1:,:].view(-1).cpu()\n",
    "            last_h = torch.tensor(maps[n].transform(Xs=last_h), dtype = torch.float32)\n",
    "            logits = torch.matmul(last_h.view(-1), model.lm_head.weight.T.cpu())\n",
    "        \n",
    "            next_token_probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "            new_object_token = pop[n]['Relation_Specificity'][j]['test_queries'][0]['answers'][0]['value']\n",
    "            new_object_token_id = tokenizer.encode(new_object_token, add_special_tokens = False)[0]\n",
    "            new_object_token_prob = next_token_probs[new_object_token_id]   \n",
    "            aliases = pop[n]['Relation_Specificity'][j]['test_queries'][0]['answers'][0]['aliases']\n",
    "            next_token_id = torch.argmax(next_token_probs, dim = -1)\n",
    "            next_token = tokenizer.decode(next_token_id).strip()\n",
    "            \n",
    "            # check if next_token is at the beginning of the string of the new_objec_token or in any of the elements of aliases, which is a list of strings\n",
    "            if any(s.startswith(next_token) for s in [new_object_token] + aliases) and next_token != \"\":\n",
    "                rs_success += 1\n",
    "            \n",
    "print(\"Total number of edits evaluated:\", count)\n",
    "print(\"Specificity score: \", rs_success)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sa test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.96it/s]\n"
     ]
    }
   ],
   "source": [
    "sa_idxs = []\n",
    "for n in tqdm(range(indexes[0], indexes[1])):\n",
    "    idxs = []\n",
    "    for j in range(len(pop[n]['Subject_Aliasing'])):\n",
    "        cond_prompt = pop[n]['Subject_Aliasing'][j]['condition_queries'][0]['prompt']\n",
    "        cond_input_ids = tokenizer(cond_prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            cond_output = model(cond_input_ids)\n",
    "        cond_last_h = cond_output.hidden_states[-1][:,-1,:].view(-1).cpu()\n",
    "        cond_last_h = torch.tensor(maps[n].transform(Xs=cond_last_h), dtype = torch.float32)\n",
    "        cond_logits = torch.matmul(cond_last_h.view(-1), model.lm_head.weight.T.cpu())\n",
    "        cond_next_token_probs = torch.softmax(cond_logits, dim=-1)\n",
    "        cond_new_object_token = pop[n]['Subject_Aliasing'][j]['condition_queries'][0]['answers'][0]['value']\n",
    "        aliases = pop[n]['Subject_Aliasing'][j]['condition_queries'][0]['answers'][0]['aliases']\n",
    "        cond_next_token = tokenizer.decode(torch.argmax(cond_next_token_probs, dim = -1)).strip()\n",
    "        if any(s.startswith(cond_next_token) for s in [cond_new_object_token] + aliases) and cond_next_token != \"\":\n",
    "            idxs.append(j)\n",
    "    sa_idxs.append(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sa_argmax_success = 0\n",
    "count = 0\n",
    "for n in tqdm(range(indexes[0],indexes[1])):\n",
    "    if n not in prob_indexes:\n",
    "        continue\n",
    "    if not pop[n]['Subject_Aliasing']:\n",
    "        continue\n",
    "    for j in range(len(pop[n]['Subject_Aliasing'])):\n",
    "        if j not in sa_idxs[n]:\n",
    "            continue\n",
    "        count += 1\n",
    "        prompt = pop[n]['Subject_Aliasing'][j]['test_queries'][0]['prompt']\n",
    "        prompt_enc = emb_model.encode(prompt)\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").to(device).input_ids\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            output = model(input_ids)\n",
    "\n",
    "            last_h = output.hidden_states[-1][:,-1:,:].view(-1).cpu()\n",
    "            pre_sim, max_index = prompt_threshold(prompt_enc, [prompt_embs[n]], t, dist_type = \"euc\")\n",
    "            if max_index is not None:\n",
    "                last_h = torch.tensor(maps[n].transform(Xs=last_h), dtype = torch.float32)\n",
    "\n",
    "            logits = torch.matmul(last_h.view(-1), model.lm_head.weight.T.cpu())\n",
    "        \n",
    "\n",
    "        next_token_probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        new_object_token = pop[n]['Subject_Aliasing'][j]['test_queries'][0]['answers'][0]['value']\n",
    "        new_object_token_id = tokenizer.encode(new_object_token, add_special_tokens = False)[0]\n",
    "        new_object_token_prob = next_token_probs[new_object_token_id]   \n",
    "        aliases = pop[n]['Subject_Aliasing'][j]['test_queries'][0]['answers'][0]['aliases']\n",
    "        next_token_id = torch.argmax(next_token_probs, dim = -1)\n",
    "        next_token = tokenizer.decode(next_token_id).strip()\n",
    "            \n",
    "        # check if next_token is at the beginning of the string of the new_objec_token or in any of the elements of aliases, which is a list of strings\n",
    "        if any(s.startswith(next_token) for s in [new_object_token] + aliases) and next_token != \"\":\n",
    "            sa_argmax_success += 1\n",
    "print(sa_argmax_success, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
